{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Avoid Data Leakage When Performing Data Preparation for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data_leakage_water](data_leakge.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation is the process of transforming raw data into a form that is appropriate for modeling.\n",
    "\n",
    "A **incorrect** approach to preparing data applies the transform on the entire rows of dataset before  fitting and evaluating the performance of the model. This results in a problem referred to as **data leakage**, where knowledge of the hold-out test set leaks into the dataset used to train the model. This can result in an incorrect estimate of model performance when making predictions on new data.\n",
    "\n",
    "A careful application of data preparation techniques is required in order to avoid data leakage, and this varies depending on the model evaluation scheme used, such as train-test splits or k-fold cross-validation.\n",
    "\n",
    "In this notebook, you will discover how to avoid data leakage during data preparation when evaluating machine learning models.\n",
    "\n",
    "After going through this notebook , you will know:\n",
    "\n",
    "- Inexperienced application of data preparation methods to the whole dataset(ie considering all the rows of raw dataset) results in data leakage that causes incorrect estimates of model performance.\n",
    "- Data preparation must be prepared on the training set only in order to avoid data leakage.\n",
    "- How to implement data preparation without data leakage for train-test splits and k-fold cross-validation in Python.\n",
    "\n",
    "\n",
    "Let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Notebook Overview\n",
    "\n",
    "This notebook is divided into three parts; they are:\n",
    "\n",
    "1. Problem With Naive(Inexperienced) Data Preparation\n",
    "2. Data Preparation With Train and Test Sets\n",
    "\n",
    "   - Train-Test Evaluation With In-correct Data Preparation\n",
    "   - Train-Test Evaluation With Correct Data Preparation\n",
    "   \n",
    "3. Data Preparation With k-fold Cross-Validation\n",
    "    - Cross-Validation Evaluation With In-correct Data Preparation\n",
    "    - Cross-Validation Evaluation With Correct Data Preparation\n",
    "    \n",
    "    \n",
    "### Problem With Incorrect Data Preparation\n",
    "The manner in which data preparation techniques are applied to data matters.\n",
    "\n",
    "A common wrong approach is to first apply one or more transforms to the entire dataset. Then the dataset is split into train and test sets or k-fold cross-validation is used to fit and evaluate a machine learning model.\n",
    "\n",
    "1. Prepare Dataset\n",
    "2. Split Data\n",
    "3. Evaluate Models\n",
    "\n",
    " This is a common wrong approach, it is dangerously incorrect in most cases.\n",
    "\n",
    "The problem with applying data preparation techniques before splitting data for train and model evaluation is that it can lead to data leakage and, in turn, will likely result in an incorrect estimate of a model’s performance on the problem.\n",
    "\n",
    "Data leakage refers to a problem where information about the holdout dataset, such as a test or validation dataset, is made available to the model in the training dataset. This leakage is often small and subtle but can have a marked effect on performance.\n",
    "\n",
    ">… leakage means that information is revealed to the model that gives it an unrealistic advantage to make better predictions. This could happen when test data is leaked into the training set, or when data from the future is leaked to the past. Any time that a model is given information that it shouldn’t have access to when it is making predictions in real time in production, there is leakage.\n",
    "\n",
    "> — Page 93, [Feature Engineering for Machine Learning](https://www.amazon.in/Feature-Engineering-Machine-Learning-Principles-ebook/dp/B07BNX4MWC/ref=dp_kinw_strp_1), 2018.\n",
    "\n",
    "We get data leakage by applying data preparation techniques to the entire dataset(ie all the rows of data set).\n",
    "\n",
    "This is not a direct type of data leakage, where we would train the model on the train+test dataset. Instead, it is an indirect type of data leakage, where some knowledge about the test dataset, captured in summary statistics is available to the model during training. This can make it a harder type of data leakage to spot, especially for beginners.\n",
    "\n",
    ">One other aspect of resampling is related to the concept of information leakage which is where the test set data are used (directly or indirectly) during the training process. This can lead to overly optimistic results that do not replicate on future data points and can occur in subtle ways.\n",
    "\n",
    "> — Page 55, [Feature Engineering and Selection](https://www.amazon.in/Feature-Engineering-Selection-Practical-Predictive-ebook/dp/B07VMP371H/ref=dp_kinw_strp_1), 2019.\n",
    "\n",
    "For example, consider the case where we want to normalize a data, that is scale input variables to the range 0-1.\n",
    "\n",
    "When we normalize the input variables, this requires that we first calculate the minimum and maximum values for each variable before using these values to scale the variables. The dataset is then split into train and test datasets, but the examples in the training dataset know something about the data in the test dataset; they have been scaled by the global minimum and maximum values, so they know more about the global distribution of the variable whhen they should not.\n",
    "\n",
    "We get the same type of leakage with almost all data preparation techniques; for example, standardization estimates the mean and standard deviation values from the domain in order to scale the variables; even models that impute missing values using a model or summary statistics will draw on the full dataset to fill in values in the training dataset.\n",
    "\n",
    "**The solution is straightforward and simple**\n",
    "\n",
    "Data preparation must be fit on the training dataset only. That is, any coefficients or parameters or models prepared for the data preparation process must only use rows of data in the training dataset.\n",
    "\n",
    "Once fit, the data preparation algorithms or models can then be applied to the training dataset, and to the test dataset.\n",
    "\n",
    "1. Split Data.\n",
    "2. Fit Data Preparation on Training Dataset (`fit` method in scikit-learn).\n",
    "3. Apply Data Preparation to Train and Test Datasets(`transform` method in scikit-learn). \n",
    "4. Fit and Evaluate Models.\n",
    "\n",
    "More generally, the entire modeling pipeline must be prepared only on the training dataset to avoid data leakage. This might include data transforms, but also other techniques such feature selection, dimensionality reduction, feature engineering and more. \n",
    "\n",
    ">In order for any resampling scheme to produce performance estimates that generalize to new data, it must contain all of the steps in the modeling process that could significantly affect the model’s effectiveness.\n",
    "\n",
    "> — Pages 54-55, [Feature Engineering and Selection]( https://www.amazon.in/Feature-Engineering-Selection-Practical-Predictive-ebook/dp/B07VMP371H/ref=dp_kinw_strp_1), 2019.\n",
    "\n",
    "Now that we are little familiar with how to apply data preparation to avoid data leakage, \n",
    "\n",
    "let’s look at some worked examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation With Train and Test Sets\n",
    "In this section, we will evaluate a logistic regression model using train and test sets on a synthetic binary classification dataset where the input variables have been normalized.\n",
    "\n",
    "First, let’s define our synthetic dataset.\n",
    "\n",
    "We will use the [make_classification()](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets) function to create the dataset with 1,000 rows of data and 20 numerical input features. The example below creates the dataset and summarizes the shape of the input and output variable arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 20), (1000,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# summarize the dataset\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.292995</td>\n",
       "      <td>-4.212231</td>\n",
       "      <td>-1.288332</td>\n",
       "      <td>-2.178498</td>\n",
       "      <td>-0.645277</td>\n",
       "      <td>2.580977</td>\n",
       "      <td>0.284224</td>\n",
       "      <td>-7.182793</td>\n",
       "      <td>-1.912111</td>\n",
       "      <td>2.737295</td>\n",
       "      <td>0.813957</td>\n",
       "      <td>3.969737</td>\n",
       "      <td>-2.669398</td>\n",
       "      <td>3.346923</td>\n",
       "      <td>4.197918</td>\n",
       "      <td>0.999910</td>\n",
       "      <td>-0.302019</td>\n",
       "      <td>-4.431706</td>\n",
       "      <td>-2.826467</td>\n",
       "      <td>0.449168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.068399</td>\n",
       "      <td>5.518841</td>\n",
       "      <td>11.238977</td>\n",
       "      <td>-5.039700</td>\n",
       "      <td>-2.086784</td>\n",
       "      <td>2.149685</td>\n",
       "      <td>0.559734</td>\n",
       "      <td>15.113777</td>\n",
       "      <td>-3.071834</td>\n",
       "      <td>-2.574584</td>\n",
       "      <td>3.324576</td>\n",
       "      <td>2.067542</td>\n",
       "      <td>-5.249258</td>\n",
       "      <td>-2.154500</td>\n",
       "      <td>4.931091</td>\n",
       "      <td>1.296735</td>\n",
       "      <td>-3.186133</td>\n",
       "      <td>-3.089948</td>\n",
       "      <td>1.190299</td>\n",
       "      <td>1.620256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.731616</td>\n",
       "      <td>-0.684686</td>\n",
       "      <td>-0.981742</td>\n",
       "      <td>-2.552465</td>\n",
       "      <td>-5.270308</td>\n",
       "      <td>-1.561498</td>\n",
       "      <td>-1.169269</td>\n",
       "      <td>-2.104087</td>\n",
       "      <td>-1.131139</td>\n",
       "      <td>4.654775</td>\n",
       "      <td>-2.786596</td>\n",
       "      <td>-2.034761</td>\n",
       "      <td>2.149657</td>\n",
       "      <td>-0.134154</td>\n",
       "      <td>-1.198231</td>\n",
       "      <td>-2.720604</td>\n",
       "      <td>-0.123961</td>\n",
       "      <td>5.654297</td>\n",
       "      <td>-0.646599</td>\n",
       "      <td>-3.156530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.309107</td>\n",
       "      <td>-0.320548</td>\n",
       "      <td>-6.591664</td>\n",
       "      <td>1.070525</td>\n",
       "      <td>-4.418769</td>\n",
       "      <td>1.134274</td>\n",
       "      <td>2.340813</td>\n",
       "      <td>-5.983425</td>\n",
       "      <td>0.675917</td>\n",
       "      <td>-1.007879</td>\n",
       "      <td>-0.761441</td>\n",
       "      <td>6.866297</td>\n",
       "      <td>1.442270</td>\n",
       "      <td>1.768678</td>\n",
       "      <td>5.173661</td>\n",
       "      <td>-1.070164</td>\n",
       "      <td>-2.447064</td>\n",
       "      <td>-1.109038</td>\n",
       "      <td>-2.997035</td>\n",
       "      <td>1.993212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.488406</td>\n",
       "      <td>-3.213065</td>\n",
       "      <td>1.100805</td>\n",
       "      <td>-1.356223</td>\n",
       "      <td>5.325086</td>\n",
       "      <td>0.729179</td>\n",
       "      <td>-0.257040</td>\n",
       "      <td>-1.035284</td>\n",
       "      <td>0.478013</td>\n",
       "      <td>-0.010764</td>\n",
       "      <td>-0.227408</td>\n",
       "      <td>2.551456</td>\n",
       "      <td>0.951594</td>\n",
       "      <td>-2.914910</td>\n",
       "      <td>-2.186843</td>\n",
       "      <td>-1.089129</td>\n",
       "      <td>1.406454</td>\n",
       "      <td>3.082424</td>\n",
       "      <td>0.925835</td>\n",
       "      <td>-2.326362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1          2         3         4         5         6  \\\n",
       "0  0.292995 -4.212231  -1.288332 -2.178498 -0.645277  2.580977  0.284224   \n",
       "1 -0.068399  5.518841  11.238977 -5.039700 -2.086784  2.149685  0.559734   \n",
       "2  0.731616 -0.684686  -0.981742 -2.552465 -5.270308 -1.561498 -1.169269   \n",
       "3  2.309107 -0.320548  -6.591664  1.070525 -4.418769  1.134274  2.340813   \n",
       "4 -0.488406 -3.213065   1.100805 -1.356223  5.325086  0.729179 -0.257040   \n",
       "\n",
       "           7         8         9        10        11        12        13  \\\n",
       "0  -7.182793 -1.912111  2.737295  0.813957  3.969737 -2.669398  3.346923   \n",
       "1  15.113777 -3.071834 -2.574584  3.324576  2.067542 -5.249258 -2.154500   \n",
       "2  -2.104087 -1.131139  4.654775 -2.786596 -2.034761  2.149657 -0.134154   \n",
       "3  -5.983425  0.675917 -1.007879 -0.761441  6.866297  1.442270  1.768678   \n",
       "4  -1.035284  0.478013 -0.010764 -0.227408  2.551456  0.951594 -2.914910   \n",
       "\n",
       "         14        15        16        17        18        19  \n",
       "0  4.197918  0.999910 -0.302019 -4.431706 -2.826467  0.449168  \n",
       "1  4.931091  1.296735 -3.186133 -3.089948  1.190299  1.620256  \n",
       "2 -1.198231 -2.720604 -0.123961  5.654297 -0.646599 -3.156530  \n",
       "3  5.173661 -1.070164 -2.447064 -1.109038 -2.997035  1.993212  \n",
       "4 -2.186843 -1.089129  1.406454  3.082424  0.925835 -2.326362  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  1\n",
       "2  1\n",
       "3  0\n",
       "4  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df = pd.DataFrame(y)\n",
    "y_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example creates the dataset and confirms that the input part of the dataset has 1,000 rows and 20 columns for the 20 input variables(Gaussian distributed) and that the output variable has 1,000 examples to match the 1,000 rows of input data, one value per row.And it is binary valued with 0 and 1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can fit and evaluate our model on the scaled dataset, starting with their naive or incorrect approach.\n",
    "\n",
    "### Train-Test Evaluation With Incorrect Data Preparation\n",
    "\n",
    "The incorrect approach involves first applying the data preparation method, then splitting the data before fit and evaluating the model.\n",
    "\n",
    "We can normalize the input variables using the [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) class, which is first defined with the default configuration scaling the data to the range 0-1, then the `fit_transform()` function is called to fit the transform on the dataset and apply it to the dataset in a single step. The result is a normalized version of the input variables, where each column in the array is separately normalized (e.g. has its own minimum and maximum calculated).\n",
    "\n",
    "The complete code for this is listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.848\n"
     ]
    }
   ],
   "source": [
    "# naive approach to normalizing the data before splitting the data and evaluating the model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# standardize the dataset\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# fit the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The example above normalizes the data(data prepartion step) and then splits the data into train and test sets, a worng way of doing things.**\n",
    "\n",
    "Next, let’s explore how we might correctly prepare the data to avoid data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Evaluation With Correct Data Preparation\n",
    "\n",
    "The correct approach to performing data preparation with a train-test split evaluation is to fit the data preparation on the training set, then apply the transform to the train and test sets.\n",
    "\n",
    "This requires that we first split the data into train and test sets and then do data preparation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.455\n"
     ]
    }
   ],
   "source": [
    "# correct approach for normalizing the data after the data is split before the model is evaluated\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# define the scaler\n",
    "scaler = MinMaxScaler()\n",
    "# fit on the training dataset\n",
    "scaler.fit(X_train)\n",
    "# scale the training dataset\n",
    "X_train = scaler.transform(X_train)\n",
    "# scale the test dataset\n",
    "X_test = scaler.transform(X_test)\n",
    "# fit the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example splits the data into train and test sets, normalizes the data correctly, then fits and evaluates the model.\n",
    "\n",
    "\n",
    "In this case, we can see that the estimate for the model is about 85.455 percent, which is more accurate than the estimate with data leakage in the previous section that achieved an accuracy of 84.848 percent.\n",
    "\n",
    "We expect data leakage to result in an incorrect estimate of model performance. We would expect this to be an optimistic estimate with data leakage, e.g. better performance, although in this case, we can see that data leakage resulted in slightly worse performance. This might be because of the difficulty of the prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation With k-fold Cross-Validation\n",
    "In this section, we will evaluate a logistic regression model using [k-fold cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html) on a synthetic binary classification dataset where the input variables have been normalized.\n",
    "\n",
    "You may recall that k-fold cross-validation involves splitting a dataset into k non-overlapping groups of rows. The model is then trained on all but one group to form a training dataset and then evaluated on the held-out fold. This process is repeated so that each fold is given a chance to be used as the holdout test set. Finally, the average performance across all evaluations is reported.\n",
    "\n",
    "The k-fold cross-validation procedure generally gives a more reliable estimate of model performance than a train-test split, although it is more computationally expensive given the repeated fitting and evaluation of models.\n",
    "\n",
    "Let’s first look at **incorrect data preparation with k-fold cross-validation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Evaluation With Incorrect Data Preparation\n",
    "Incorrect data preparation with cross-validation involves applying the data transforms on the complete dataset first, then using the cross-validation procedure.\n",
    "\n",
    "We will use the synthetic dataset prepared in the previous section and normalize the data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the dataset\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-fold cross-validation procedure must first be defined. We will use repeated stratified 10-fold cross-validation, which is a best practice for classification. Repeated means that the whole cross-validation procedure is repeated multiple times, three in this case. Stratified means that each group of rows will have the relative composition of examples from each class as the whole dataset. We will use k=10 or 10-fold cross-validation.\n",
    "\n",
    "This can be achieved using the [`RepeatedStratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html) which can be configured to three repeats and 10 folds, and then using the [`cross_val_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function to perform the procedure, passing in the defined model, cross-validation object, and metric to calculate, in this case, accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86, 0.91, 0.88, 0.81, 0.83, 0.84, 0.81, 0.84, 0.88, 0.84, 0.84,\n",
       "       0.86, 0.85, 0.83, 0.89, 0.87, 0.79, 0.97, 0.84, 0.84, 0.81, 0.88,\n",
       "       0.8 , 0.85, 0.89, 0.88, 0.87, 0.83, 0.83, 0.87])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model using cross-validation\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us combine all the code pieces and form the complete example as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 85.300 and Stadard deviation: 3.607\n"
     ]
    }
   ],
   "source": [
    "# naive data preparation for model evaluation with k-fold cross-validation\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# standardize the dataset\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# define the model\n",
    "model = LogisticRegression()\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model using cross-validation\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Average Accuracy: %.3f and Stadard deviation: %.3f' % (mean(scores)*100, std(scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the  above example normalizes the data first(data preparation first), then  evaluates the model using repeated stratified cross-validation.\n",
    "\n",
    "In this case, we can see that the model achieved an estimated accuracy of about 85.300 percent, which we know is **incorrect given the data leakage** allowed via the data preparation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s look at how we can evaluate the model with cross-validation and avoid data leakage.\n",
    "\n",
    "### Cross-Validation Evaluation With Correct Data Preparation\n",
    "\n",
    "Data preparation without data leakage when using cross-validation is slightly more challenging.\n",
    "\n",
    "It requires that the data preparation method is prepared on the training set and applied to the train and test sets within the cross-validation procedure, e.g. the groups of folds of rows.\n",
    "\n",
    "We can achieve this by defining a modeling pipeline that defines a sequence of data preparation steps to perform and ending in the model to fit and evaluate.\n",
    "\n",
    ">To provide a solid methodology, we should constrain ourselves to developing the list of preprocessing techniques, estimate them only in the presence of the training data points, and then apply the techniques to future data (including the test set).\n",
    "\n",
    ">— Page 55, [Feature Engineering and Selection](https://www.amazon.in/Feature-Engineering-Selection-Practical-Predictive-ebook/dp/B07VMP371H/ref=dp_kinw_strp_1), 2019.\n",
    "\n",
    "The evaluation procedure changes from simply and incorrectly evaluating just the model to correctly evaluating the entire pipeline of data preparation and model together as a single atomic unit.\n",
    "\n",
    "This can be achieved using the [Pipeline class.](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "\n",
    "This class takes a list of steps that define the pipeline. Each step in the list is a tuple with two elements. The first element is the name of the step (a string) and the second is the configured object of the step, such as a transform or a model. The model is only supported as the final step, although we can have as many transforms as we like in the sequence."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "...\n",
    "# define the pipeline\n",
    "steps = list()\n",
    "steps.append(('scaler', MinMaxScaler()))\n",
    "steps.append(('model', LogisticRegression()))\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then pass the configured object to the cross_val_score() function for evaluation."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "...\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model using cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all this together, the complete example of correctly performing data preparation without data leakage when using cross-validation is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 85.433 and Stadard deviation: 3.471\n"
     ]
    }
   ],
   "source": [
    "# correct data preparation for model evaluation with k-fold cross-validation\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# define the pipeline\n",
    "steps = list()\n",
    "steps.append(('scaler', MinMaxScaler()))\n",
    "steps.append(('model', LogisticRegression()))\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model using cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Average Accuracy: %.3f and Stadard deviation: %.3f' % (mean(scores)*100, std(scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example normalizes the data correctly within the cross-validation folds of the evaluation procedure to avoid data leakage.\n",
    "\n",
    "In this case, we can see that the model has an estimated accuracy of about 85.433 percent, compared to the approach with data leakage that achieved an accuracy of about 85.300 percent.\n",
    "\n",
    "As with the train-test example in the previous section, removing data leakage has resulted in a slight improvement in performance when our intuition might suggest a drop given that data leakage often results in an optimistic estimate of model performance. Nevertheless, the examples clearly demonstrate that data leakage does impact the estimate of model performance and how to correct data leakage by correctly performing data preparation after the data is split.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refrences\n",
    "- [Data preparation, Wikipedia.](https://en.wikipedia.org/wiki/Data_preparation)\n",
    "- [Data cleansing, Wikipedia.](https://en.wikipedia.org/wiki/Data_cleansing)\n",
    "- [Data pre-processing, Wikipedia.](https://en.wikipedia.org/wiki/Data_pre-processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this tutorial, you discovered how to avoid data leakage during data preparation when evaluating machine learning models.\n",
    "\n",
    "Specifically, we learned:\n",
    "\n",
    "- Incorrect application of data preparation methods to the whole dataset results in data leakage that causes incorrect estimates of model performance.\n",
    "- Data preparation must be prepared on the training set only in order to avoid data leakage.\n",
    "- How to implement data preparation without data leakage for train-test splits and k-fold cross-validation in Python.\n",
    "\n",
    "\n",
    "**Do you have any questions?**\n",
    "\n",
    "Ask your questions in the comments below and I will try my best to answer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
